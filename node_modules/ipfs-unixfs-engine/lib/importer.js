'use strict';

var merkleDAG = require('ipfs-merkle-dag');
var UnixFS = require('ipfs-unixfs');
var assert = require('assert');
var pull = require('pull-stream');
var pushable = require('pull-pushable');
var write = require('pull-write');
var parallel = require('run-parallel');

var fsc = require('./chunker-fixed-size');
var createAndStoreTree = require('./tree');

var DAGNode = merkleDAG.DAGNode;

var CHUNK_SIZE = 262144;

module.exports = function (dagService, options) {
  assert(dagService, 'Missing dagService');

  var files = [];

  var source = pushable();
  var sink = write(makeWriter(source, files, dagService), null, 100, function (err) {
    if (err) return source.end(err);

    createAndStoreTree(files, dagService, source, function () {
      source.end();
    });
  });

  return { source: source, sink: sink };
};

function makeWriter(source, files, dagService) {
  return function (items, cb) {
    parallel(items.map(function (item) {
      return function (cb) {
        if (!item.content) {
          return createAndStoreDir(item, dagService, function (err, node) {
            if (err) return cb(err);
            source.push(node);
            files.push(node);
            cb();
          });
        }

        createAndStoreFile(item, dagService, function (err, node) {
          if (err) return cb(err);
          source.push(node);
          files.push(node);
          cb();
        });
      };
    }), cb);
  };
}

function createAndStoreDir(item, ds, cb) {
  // 1. create the empty dir dag node
  // 2. write it to the dag store

  var d = new UnixFS('directory');
  var n = new DAGNode();
  n.data = d.marshal();

  ds.put(n, function (err) {
    if (err) return cb(err);
    cb(null, {
      path: item.path,
      multihash: n.multihash(),
      size: n.size()
      // dataSize: d.fileSize()
    });
  });
}

function createAndStoreFile(file, ds, cb) {
  if (Buffer.isBuffer(file.content)) {
    file.content = pull.values([file.content]);
  }

  if (typeof file.content !== 'function') {
    return cb(new Error('invalid content'));
  }

  // 1. create the unixfs merkledag node
  // 2. add its hash and size to the leafs array

  // TODO - Support really large files
  // a) check if we already reach max chunks if yes
  // a.1) create a parent node for all of the current leaves
  // b.2) clean up the leaves array and add just the parent node

  pull(file.content, fsc(CHUNK_SIZE), pull.asyncMap(function (chunk, cb) {
    var l = new UnixFS('file', Buffer(chunk));
    var n = new DAGNode(l.marshal());

    ds.put(n, function (err) {
      if (err) {
        return cb(new Error('Failed to store chunk'));
      }

      cb(null, {
        Hash: n.multihash(),
        Size: n.size(),
        leafSize: l.fileSize(),
        Name: ''
      });
    });
  }), pull.collect(function (err, leaves) {
    if (err) return cb(err);

    if (leaves.length === 1) {
      return cb(null, {
        path: file.path,
        multihash: leaves[0].Hash,
        size: leaves[0].Size
        // dataSize: leaves[0].leafSize
      });
    }

    // create a parent node and add all the leafs

    var f = new UnixFS('file');
    var n = new merkleDAG.DAGNode();

    var _iteratorNormalCompletion = true;
    var _didIteratorError = false;
    var _iteratorError = undefined;

    try {
      for (var _iterator = leaves[Symbol.iterator](), _step; !(_iteratorNormalCompletion = (_step = _iterator.next()).done); _iteratorNormalCompletion = true) {
        var leaf = _step.value;

        f.addBlockSize(leaf.leafSize);
        n.addRawLink(new merkleDAG.DAGLink(leaf.Name, leaf.Size, leaf.Hash));
      }
    } catch (err) {
      _didIteratorError = true;
      _iteratorError = err;
    } finally {
      try {
        if (!_iteratorNormalCompletion && _iterator.return) {
          _iterator.return();
        }
      } finally {
        if (_didIteratorError) {
          throw _iteratorError;
        }
      }
    }

    n.data = f.marshal();
    ds.put(n, function (err) {
      if (err) return cb(err);

      cb(null, {
        path: file.path,
        multihash: n.multihash(),
        size: n.size()
        // dataSize: f.fileSize()
      });
    });
  }));
}